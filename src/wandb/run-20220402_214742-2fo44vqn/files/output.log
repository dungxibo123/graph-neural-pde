GrandExtendDiscritizedNet
m1.weight
torch.Size([80, 3703])
m1.bias
torch.Size([80])
m2.weight
torch.Size([6, 80])
m2.bias
torch.Size([6])
mol_list.0.alpha_train
torch.Size([])
mol_list.0.beta_train
torch.Size([])
mol_list.0.alpha_sc
torch.Size([1])
mol_list.0.beta_sc
torch.Size([1])
mol_list.0.multihead_att_layer.Q.weight
torch.Size([32, 80])
mol_list.0.multihead_att_layer.Q.bias
torch.Size([32])
mol_list.0.multihead_att_layer.V.weight
torch.Size([32, 80])
mol_list.0.multihead_att_layer.V.bias
torch.Size([32])
mol_list.0.multihead_att_layer.K.weight
torch.Size([32, 80])
mol_list.0.multihead_att_layer.K.bias
torch.Size([32])
mol_list.0.multihead_att_layer.Wout.weight
torch.Size([80, 4])
mol_list.0.multihead_att_layer.Wout.bias
torch.Size([80])
mol_list.1.alpha_train
torch.Size([])
mol_list.1.beta_train
torch.Size([])
mol_list.1.alpha_sc
torch.Size([1])
mol_list.1.beta_sc
torch.Size([1])
mol_list.1.multihead_att_layer.Q.weight
torch.Size([32, 80])
mol_list.1.multihead_att_layer.Q.bias
torch.Size([32])
mol_list.1.multihead_att_layer.V.weight
torch.Size([32, 80])
mol_list.1.multihead_att_layer.V.bias
torch.Size([32])
mol_list.1.multihead_att_layer.K.weight
torch.Size([32, 80])
mol_list.1.multihead_att_layer.K.bias
torch.Size([32])
mol_list.1.multihead_att_layer.Wout.weight
torch.Size([80, 4])
mol_list.1.multihead_att_layer.Wout.bias
torch.Size([80])
mol_list.2.alpha_train
torch.Size([])
mol_list.2.beta_train
torch.Size([])
mol_list.2.alpha_sc
torch.Size([1])
mol_list.2.beta_sc
torch.Size([1])
mol_list.2.multihead_att_layer.Q.weight
torch.Size([32, 80])
mol_list.2.multihead_att_layer.Q.bias
torch.Size([32])
mol_list.2.multihead_att_layer.V.weight
torch.Size([32, 80])
mol_list.2.multihead_att_layer.V.bias
torch.Size([32])
mol_list.2.multihead_att_layer.K.weight
torch.Size([32, 80])
mol_list.2.multihead_att_layer.K.bias
torch.Size([32])
mol_list.2.multihead_att_layer.Wout.weight
torch.Size([80, 4])
mol_list.2.multihead_att_layer.Wout.bias
torch.Size([80])
mol_list.3.alpha_train
torch.Size([])
mol_list.3.beta_train
torch.Size([])
mol_list.3.alpha_sc
torch.Size([1])
mol_list.3.beta_sc
torch.Size([1])
mol_list.3.multihead_att_layer.Q.weight
torch.Size([32, 80])
mol_list.3.multihead_att_layer.Q.bias
torch.Size([32])
mol_list.3.multihead_att_layer.V.weight
torch.Size([32, 80])
mol_list.3.multihead_att_layer.V.bias
torch.Size([32])
mol_list.3.multihead_att_layer.K.weight
torch.Size([32, 80])
mol_list.3.multihead_att_layer.K.bias
torch.Size([32])
mol_list.3.multihead_att_layer.Wout.weight
torch.Size([80, 4])
mol_list.3.multihead_att_layer.Wout.bias
torch.Size([80])
  0%|                                      | 0/250 [00:00<?, ?it/s]
Epoch: 001, Runtime 1.404130, Loss 591701357853933568.000000, Train: 0.1667, Val: 0.1368, Test: 0.0833, Best time: 7.8741
Epoch: 002, Runtime 0.025558, Loss 275656825032933376.000000, Train: 0.1467, Val: 0.1130, Test: 0.0833, Best time: 7.8741
Epoch: 003, Runtime 0.027768, Loss 170985104775053312.000000, Train: 0.1533, Val: 0.1114, Test: 0.0750, Best time: 7.8741
Epoch: 004, Runtime 0.027081, Loss 182970073575587840.000000, Train: 0.1467, Val: 0.1070, Test: 0.1000, Best time: 7.8741
Epoch: 005, Runtime 0.025751, Loss 180541544447606784.000000, Train: 0.1533, Val: 0.1097, Test: 0.0917, Best time: 7.8741
Epoch: 006, Runtime 0.027290, Loss 235446791451967488.000000, Train: 0.1467, Val: 0.1157, Test: 0.0833, Best time: 7.8741
Epoch: 007, Runtime 0.027339, Loss 125283869606805504.000000, Train: 0.1600, Val: 0.1335, Test: 0.0667, Best time: 7.8741
Epoch: 008, Runtime 0.044352, Loss 120083042168471552.000000, Train: 0.1667, Val: 0.1432, Test: 0.0833, Best time: 7.8741
Epoch: 009, Runtime 0.030799, Loss 75799017358884864.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 010, Runtime 0.029391, Loss 96184615772880896.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 011, Runtime 0.033593, Loss 95692369571086336.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 012, Runtime 0.028043, Loss 55715007253446656.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 013, Runtime 0.027763, Loss 65973102848245760.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 014, Runtime 0.026761, Loss 82751873626406912.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 015, Runtime 0.028267, Loss 49034645186019328.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 016, Runtime 0.030313, Loss 66269258023174144.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 017, Runtime 0.030848, Loss 45053244862365696.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 018, Runtime 0.031535, Loss 62443691997921280.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 019, Runtime 0.030907, Loss 64198014339645440.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 020, Runtime 0.029223, Loss 30983284187987968.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 021, Runtime 0.027732, Loss 47182251561058304.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 022, Runtime 0.027251, Loss 54954003473104896.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 023, Runtime 0.030457, Loss 41356424776777728.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 024, Runtime 0.030150, Loss 28177528082399232.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 025, Runtime 0.044740, Loss 54643966963875840.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 026, Runtime 0.028718, Loss 52805076716093440.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 027, Runtime 0.027174, Loss 42796961102823424.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 028, Runtime 0.033945, Loss 31715453705388032.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 029, Runtime 0.029269, Loss 38603741582065664.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 030, Runtime 0.033924, Loss 24881791370264576.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 031, Runtime 0.025717, Loss 27252563925532672.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 032, Runtime 0.025300, Loss 34622744985337856.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 033, Runtime 0.025815, Loss 20010224714776576.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 034, Runtime 0.026697, Loss 25304959465553920.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 035, Runtime 0.027797, Loss 30476619780980736.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 036, Runtime 0.027501, Loss 32635107135193088.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 037, Runtime 0.027748, Loss 45704636782346240.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 038, Runtime 0.027781, Loss 14024249337446400.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 039, Runtime 0.026797, Loss 29069616412098560.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 040, Runtime 0.025885, Loss 36868725118337024.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 041, Runtime 0.025397, Loss 26036791828021248.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 042, Runtime 0.037615, Loss 21956929379106816.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 043, Runtime 0.028762, Loss 16396277096906752.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 044, Runtime 0.028754, Loss 18893803800756224.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 045, Runtime 0.028877, Loss 13293354351591424.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 046, Runtime 0.026626, Loss 18217915534802944.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 047, Runtime 0.028180, Loss 22925560468471808.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 048, Runtime 0.032462, Loss 17269064917319680.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 049, Runtime 0.029083, Loss 22032312497602560.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 050, Runtime 0.027473, Loss 12134332730703872.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 051, Runtime 0.031889, Loss 21316008589393920.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 052, Runtime 0.032191, Loss 15073507700375552.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 053, Runtime 0.027996, Loss 10421345154236416.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 054, Runtime 0.030455, Loss 12803999736528896.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 055, Runtime 0.032591, Loss 13738243170238464.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 056, Runtime 0.026849, Loss 17447496112406528.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 057, Runtime 0.027156, Loss 10991876798676992.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741

 29%|████████▎                    | 72/250 [00:03<00:05, 34.00it/s]
Epoch: 059, Runtime 0.040342, Loss 14472898399961088.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 060, Runtime 0.028175, Loss 17289464938233856.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 061, Runtime 0.030000, Loss 7968118353690624.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 062, Runtime 0.028265, Loss 14146022397706240.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 063, Runtime 0.027643, Loss 6248304463577088.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 064, Runtime 0.029961, Loss 17693118849613824.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 065, Runtime 0.025488, Loss 6279070388060160.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 066, Runtime 0.026511, Loss 8486502652706816.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 067, Runtime 0.026459, Loss 9868160882704384.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 068, Runtime 0.025702, Loss 18948740727439360.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 069, Runtime 0.030598, Loss 9645913505005568.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 070, Runtime 0.027388, Loss 10405176749850624.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 071, Runtime 0.028327, Loss 11041948601155584.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 072, Runtime 0.029134, Loss 12609185388691456.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 073, Runtime 0.028288, Loss 10273784036589568.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 074, Runtime 0.027278, Loss 14144947582140416.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 075, Runtime 0.052753, Loss 10968458489495552.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 076, Runtime 0.044621, Loss 11103565275725824.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 077, Runtime 0.026650, Loss 12142436260249600.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 078, Runtime 0.029013, Loss 13994823442759680.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 079, Runtime 0.031275, Loss 12654746401767424.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 080, Runtime 0.027015, Loss 5203326572429312.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 081, Runtime 0.026881, Loss 10033670668681216.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 082, Runtime 0.032456, Loss 10722661806112768.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 083, Runtime 0.028843, Loss 9675167601000448.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 084, Runtime 0.029055, Loss 6387561798828032.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 085, Runtime 0.028755, Loss 12006595034611712.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 086, Runtime 0.027646, Loss 7763272035991552.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 087, Runtime 0.028440, Loss 10280897576173568.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 088, Runtime 0.026848, Loss 6660779164041216.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 089, Runtime 0.031733, Loss 4847934134812672.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 090, Runtime 0.027774, Loss 4553835175477248.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 091, Runtime 0.028302, Loss 6830045972660224.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 092, Runtime 0.028256, Loss 13313609417359360.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 093, Runtime 0.043939, Loss 10901156351967232.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 094, Runtime 0.029671, Loss 9396313628082176.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 095, Runtime 0.028645, Loss 5032648867053568.000000, Train: 0.1667, Val: 0.1449, Test: 0.0833, Best time: 7.8741
Epoch: 096, Runtime 0.029381, Loss 17174512286040064.000000, Train: 0.1733, Val: 0.1492, Test: 0.1083, Best time: 7.8741
Epoch: 097, Runtime 0.027486, Loss 8921074423037952.000000, Train: 0.2133, Val: 0.2005, Test: 0.1667, Best time: 7.8741
Epoch: 098, Runtime 0.029230, Loss 8623665654530048.000000, Train: 0.1933, Val: 0.2676, Test: 0.2750, Best time: 7.8741
Epoch: 099, Runtime 0.028175, Loss 4465852501983232.000000, Train: 0.1733, Val: 0.2605, Test: 0.2750, Best time: 7.8741
Epoch: 100, Runtime 0.028444, Loss 3954327265738752.000000, Train: 0.1667, Val: 0.2573, Test: 0.2833, Best time: 7.8741
Epoch: 101, Runtime 0.030097, Loss 6277930611113984.000000, Train: 0.1667, Val: 0.2562, Test: 0.2833, Best time: 7.8741
Epoch: 102, Runtime 0.028980, Loss 5192541909549056.000000, Train: 0.1667, Val: 0.2562, Test: 0.2833, Best time: 7.8741
Epoch: 103, Runtime 0.028710, Loss 2952798605934592.000000, Train: 0.1667, Val: 0.2562, Test: 0.2833, Best time: 7.8741
Epoch: 104, Runtime 0.027020, Loss 8590368920567808.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 105, Runtime 0.028042, Loss 10288878699151360.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 106, Runtime 0.027915, Loss 3084211451854848.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 107, Runtime 0.025736, Loss 3487418686636032.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 108, Runtime 0.030752, Loss 8371117282557952.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 109, Runtime 0.030982, Loss 2878376486371328.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 110, Runtime 0.166947, Loss 5467128295587840.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 111, Runtime 0.031312, Loss 3662763679285248.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 112, Runtime 0.028681, Loss 6841504945405952.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 113, Runtime 0.030818, Loss 9674928156573696.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 114, Runtime 0.028666, Loss 3102398222434304.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 115, Runtime 0.032596, Loss 4447913497329664.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 116, Runtime 0.027659, Loss 4277953319927808.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 117, Runtime 0.030677, Loss 4762377312534528.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 118, Runtime 0.026348, Loss 3802049468694528.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 119, Runtime 0.028573, Loss 2513416472231936.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 120, Runtime 0.027228, Loss 8418252938018816.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 121, Runtime 0.025681, Loss 498743849779200.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 122, Runtime 0.025833, Loss 3122341903073280.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 123, Runtime 0.027267, Loss 4176507266138112.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 124, Runtime 0.026932, Loss 3753676057346048.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 125, Runtime 0.027492, Loss 9025616074506240.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741

 54%|███████████████▏            | 136/250 [00:05<00:03, 33.96it/s]
Epoch: 127, Runtime 0.033253, Loss 2339494724370432.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 128, Runtime 0.027000, Loss 697050576453632.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 129, Runtime 0.025248, Loss 3635228643950592.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 130, Runtime 0.025208, Loss 2016718671052800.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 131, Runtime 0.027277, Loss 2113911901913088.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 132, Runtime 0.029550, Loss 6079060203536384.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 133, Runtime 0.027782, Loss 2021789148381184.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 134, Runtime 0.028152, Loss 649608703246336.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 135, Runtime 0.027048, Loss 1977473004732416.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 136, Runtime 0.025970, Loss 2612448754401280.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 137, Runtime 0.028120, Loss 927921644503040.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 138, Runtime 0.056450, Loss 8046383932112896.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 139, Runtime 0.028469, Loss 3079637311684608.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 140, Runtime 0.026349, Loss 4620018474024960.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 141, Runtime 0.026657, Loss 12418242953871360.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 142, Runtime 0.027339, Loss 1817757800726528.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 143, Runtime 0.025048, Loss 3072047030730752.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 144, Runtime 0.045272, Loss 1779985576624128.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 145, Runtime 0.032981, Loss 2610376701116416.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 146, Runtime 0.026959, Loss 2648137248276480.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 147, Runtime 0.029068, Loss 1935901613621248.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 148, Runtime 0.027958, Loss 2121529965936640.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 149, Runtime 0.025730, Loss 6138855811973120.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 150, Runtime 0.027639, Loss 2929789392388096.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 151, Runtime 0.027486, Loss 2246541163102208.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 152, Runtime 0.029294, Loss 2533595033894912.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 153, Runtime 0.028273, Loss 2192069569282048.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 154, Runtime 0.030481, Loss 4296124521250816.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 155, Runtime 0.026695, Loss 4849188265263104.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 156, Runtime 0.025936, Loss 1396771951476736.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 157, Runtime 0.028422, Loss 1968135880048640.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 158, Runtime 0.027394, Loss 3094843374960640.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 159, Runtime 0.029677, Loss 1477288226979840.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 160, Runtime 0.027329, Loss 2543688743911424.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 161, Runtime 0.044333, Loss 1546709930868736.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 162, Runtime 0.030966, Loss 1345728009994240.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 163, Runtime 0.031080, Loss 3565973638479872.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 164, Runtime 0.028495, Loss 2079040659783680.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 165, Runtime 0.029824, Loss 1764203450859520.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 166, Runtime 0.028310, Loss 997385224323072.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 167, Runtime 0.031225, Loss 2058920214396928.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 168, Runtime 0.026103, Loss 1587168657014784.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 169, Runtime 0.027261, Loss 2314681356124160.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 170, Runtime 0.028128, Loss 3453749901131776.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 171, Runtime 0.029757, Loss 942038933569536.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 172, Runtime 0.027185, Loss 811233959739392.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 173, Runtime 0.027931, Loss 1362015465504768.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 174, Runtime 0.028364, Loss 1653475301654528.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 175, Runtime 0.029367, Loss 1059618360918016.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 176, Runtime 0.026379, Loss 2055551886295040.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 177, Runtime 0.028860, Loss 1918704430350336.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 178, Runtime 0.050537, Loss 2874986951868416.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 179, Runtime 0.030567, Loss 1859256848482304.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 180, Runtime 0.029137, Loss 1534143930302464.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 181, Runtime 0.028460, Loss 4085798899023872.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 182, Runtime 0.029095, Loss 1147405076529152.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 183, Runtime 0.029905, Loss 1611228258500608.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 184, Runtime 0.030668, Loss 982634360471552.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 185, Runtime 0.027814, Loss 2153462242476032.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 186, Runtime 0.026217, Loss 1011187839926272.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 187, Runtime 0.028129, Loss 2375443600637952.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 188, Runtime 0.026898, Loss 1025660067774464.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 189, Runtime 0.027472, Loss 2370218236051456.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 190, Runtime 0.024789, Loss 1369229668384768.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 191, Runtime 0.027043, Loss 1584324986011648.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 192, Runtime 0.028633, Loss 3235932245327872.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 193, Runtime 0.029362, Loss 3869106793086976.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741


100%|████████████████████████████| 250/250 [00:09<00:00, 27.69it/s]
Epoch: 195, Runtime 0.038971, Loss 894217425518592.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 196, Runtime 0.037019, Loss 1757611884019712.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 197, Runtime 0.031047, Loss 1607938581987328.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 198, Runtime 0.030605, Loss 2380091292123136.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 199, Runtime 0.028543, Loss 1990568896888832.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 200, Runtime 0.029222, Loss 721996551815168.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 201, Runtime 0.026723, Loss 1699550100193280.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 202, Runtime 0.028277, Loss 2942551619272704.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 203, Runtime 0.026955, Loss 1178463193006080.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 204, Runtime 0.055747, Loss 1437861635162112.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 205, Runtime 0.031011, Loss 2488765507436544.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 206, Runtime 0.025848, Loss 1642700302450688.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 207, Runtime 0.024291, Loss 2277175285776384.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 208, Runtime 0.027131, Loss 3244026111197184.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 209, Runtime 0.027763, Loss 960044845760512.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 210, Runtime 0.025178, Loss 1112194733309952.000000, Train: 0.1667, Val: 0.2557, Test: 0.2833, Best time: 7.8741
Epoch: 211, Runtime 0.025894, Loss 1729129808396288.000000, Train: 0.1667, Val: 0.1443, Test: 0.0917, Best time: 7.8741
Epoch: 212, Runtime 0.036473, Loss 1803896733302784.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 213, Runtime 0.026058, Loss 1727291830829056.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 214, Runtime 0.031581, Loss 1554698435821568.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 215, Runtime 0.026680, Loss 1519102686396416.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 216, Runtime 0.027876, Loss 1231466310664192.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 217, Runtime 0.027822, Loss 1564929182138368.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 218, Runtime 0.029014, Loss 1455895129096192.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 219, Runtime 0.030035, Loss 3477912313397248.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 220, Runtime 0.026864, Loss 1712053286862848.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 221, Runtime 0.029090, Loss 1148492105908224.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 222, Runtime 0.027115, Loss 1113945268027392.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 223, Runtime 0.028457, Loss 1858537709895680.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 224, Runtime 0.026423, Loss 2522975190384640.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 225, Runtime 0.024793, Loss 1030115291037696.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 226, Runtime 0.025558, Loss 2423579178172416.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 227, Runtime 0.025325, Loss 1312539187085312.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 228, Runtime 0.024906, Loss 1716274300190720.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 229, Runtime 0.027374, Loss 1912113266163712.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 230, Runtime 0.043731, Loss 2986849643528192.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 231, Runtime 0.027974, Loss 1169026478768128.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 232, Runtime 0.028959, Loss 1115984370860032.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 233, Runtime 0.027314, Loss 1677862125961216.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 234, Runtime 0.029993, Loss 1072440046256128.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 235, Runtime 0.029602, Loss 2033271206576128.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 236, Runtime 0.026115, Loss 3361726468718592.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 237, Runtime 0.027581, Loss 892686940766208.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 238, Runtime 0.033931, Loss 2259496126644224.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 239, Runtime 0.028109, Loss 2129712549724160.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 240, Runtime 0.031703, Loss 882136085168128.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 241, Runtime 0.029884, Loss 1344674266611712.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 242, Runtime 0.028580, Loss 3260054660710400.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 243, Runtime 0.028424, Loss 1194593747992576.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 244, Runtime 0.031834, Loss 1492497243045888.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 245, Runtime 0.030216, Loss 1790810773258240.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 246, Runtime 0.036875, Loss 1871007207915520.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 247, Runtime 0.031554, Loss 1010422530441216.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
Epoch: 248, Runtime 0.029409, Loss 1485278208327680.000000, Train: 0.1667, Val: 0.1459, Test: 0.0833, Best time: 7.8741
Epoch: 249, Runtime 0.030571, Loss 585858973433856.000000, Train: 0.1667, Val: 0.1459, Test: 0.0833, Best time: 7.8741
Epoch: 250, Runtime 0.028330, Loss 1125504434307072.000000, Train: 0.1667, Val: 0.1454, Test: 0.0833, Best time: 7.8741
best val accuracy 0.145405 with test accuracy 0.083333 at epoch 98 and best time 7.874113